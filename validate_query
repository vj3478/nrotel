import json
import requests
import csv
from pathlib import Path

# Configuration
GRAPHQL_URL = "https://api.newrelic.com/graphql"
API_KEY = "YOUR_NEW_RELIC_API_KEY"
INPUT_JSON = "dashboard.json"
EXTRACTED_CSV = "extracted_queries.csv"
VALIDATED_CSV = "validated_dashboard_queries.csv"
BATCH_SIZE = 25

# Step 1: Extract rawConfiguration and write to CSV
def extract_nrql_to_csv(json_file, csv_file):
    with open(json_file) as f:
        data = json.load(f)

    rows = []
    pages = data["data"]["actor"]["entity"].get("pages", [])
    for page in pages:
        for widget in page.get("widgets", []):
            config = widget.get("rawConfiguration", {})
            account_id = config.get("accountId")
            queries = config.get("nrqlQueries", [])

            # Support multiple account IDs
            if isinstance(account_id, list):
                account_id_str = ",".join(map(str, account_id))
            else:
                account_id_str = str(account_id)

            for q in queries:
                if "query" in q:
                    rows.append({
                        "accountId": account_id_str,
                        "nrqlQuery": q["query"]
                    })

    if rows:
        with open(csv_file, mode="w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)

    print(f"✅ Extracted {len(rows)} queries to {csv_file}")


# Step 2: Read CSV and batch
def load_and_batch_csv(csv_file, batch_size=25):
    with open(csv_file, newline='') as f:
        rows = list(csv.DictReader(f))
    return [rows[i:i + batch_size] for i in range(0, len(rows), batch_size)]


# Step 3: Build GraphQL query
def build_graphql_query(batch):
    parts = []
    for i, row in enumerate(batch):
        alias = f"q{i}"
        account_list = [int(a.strip()) for a in row["accountId"].split(",")]
        nrql = row["nrqlQuery"].replace('"', '\\"').replace('\n', ' ').replace('\r', '')
        parts.append(f"""
            {alias}: actor {{
                nrql(accounts: {account_list}, query: "{nrql}") {{
                    results
                    metadata {{ eventTypes facets accounts }}
                }}
            }}
        """)
    return "query {\n" + "\n".join(parts) + "\n}"


# Step 4: Validate queries
def validate_batches(batches):
    headers = {
        "Api-Key": API_KEY,
        "Content-Type": "application/json"
    }
    validated_rows = []

    for batch in batches:
        gql = build_graphql_query(batch)
        response = requests.post(GRAPHQL_URL, headers=headers, json={"query": gql})
        try:
            data = response.json()
            error_paths = {err["path"][0] for err in data.get("errors", [])} if "errors" in data else set()
        except Exception as e:
            print("Error parsing response:", e)
            error_paths = set(f"q{i}" for i in range(len(batch)))

        for i, row in enumerate(batch):
            alias = f"q{i}"
            row["result"] = "failed" if alias in error_paths else row["nrqlQuery"]
            validated_rows.append(row)

    return validated_rows


# Step 5: Save final output
def save_validated_to_csv(rows, output_file):
    with open(output_file, mode='w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)
    print(f"✅ Validation results written to: {output_file}")


# Main flow
def main():
    extract_nrql_to_csv(INPUT_JSON, EXTRACTED_CSV)
    batches = load_and_batch_csv(EXTRACTED_CSV, BATCH_SIZE)
    validated = validate_batches(batches)
    save_validated_to_csv(validated, VALIDATED_CSV)

if __name__ == "__main__":
    main()
