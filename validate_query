import csv
import json
import requests

# Configuration
GRAPHQL_URL = "https://api.newrelic.com/graphql"
API_KEY = "YOUR_NEW_RELIC_API_KEY"  # üîê Replace with your actual key
BATCH_SIZE = 25
INPUT_FILE = "nrql_queries.csv"
OUTPUT_FILE = "validated_nrql_queries.csv"

# Load input CSV
def load_nrql_queries(file_path):
    with open(file_path, newline='') as f:
        return list(csv.DictReader(f))

# Batch the queries
def batch_queries(queries, size):
    return [queries[i:i + size] for i in range(0, len(queries), size)]

# Build GraphQL query that supports multiple accounts per query
def build_graphql_query(batch):
    parts = []
    for i, row in enumerate(batch):
        alias = f"q{i}"

        # Handle single or multiple account IDs
        account_ids = row["accountId"]
        if isinstance(account_ids, str):
            account_list = [int(a.strip()) for a in account_ids.split(",")]
        else:
            account_list = account_ids

        nrql = row["nrqlQuery"].replace('"', '\\"').replace('\n', ' ').replace('\r', '')
        
        parts.append(f"""
            {alias}: actor {{
                nrql(accounts: {account_list}, query: "{nrql}") {{
                    results
                    metadata {{
                        eventTypes
                        facets
                        accounts
                    }}
                }}
            }}
        """)
    return "query {\n" + "\n".join(parts) + "\n}"

# Send queries and process results
def verify_nrql_batches(batches):
    headers = {
        "Api-Key": API_KEY,
        "Content-Type": "application/json"
    }
    results = []

    for batch in batches:
        gql = build_graphql_query(batch)
        response = requests.post(GRAPHQL_URL, headers=headers, json={"query": gql})
        try:
            data = response.json()
            error_paths = {err["path"][0] for err in data.get("errors", [])} if "errors" in data else set()
        except Exception as e:
            print("Error parsing response:", e)
            error_paths = set(f"q{i}" for i in range(len(batch)))

        for i, row in enumerate(batch):
            alias = f"q{i}"
            row["result"] = "failed" if alias in error_paths else row["nrqlQuery"]
            results.append(row)

    return results

# Save output
def save_to_csv(data, path):
    fieldnames = list(data[0].keys())
    with open(path, mode='w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)

# Main driver
def main():
    queries = load_nrql_queries(INPUT_FILE)
    batches = batch_queries(queries, BATCH_SIZE)
    validated = verify_nrql_batches(batches)
    save_to_csv(validated, OUTPUT_FILE)
    print(f"Validation complete. Output saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
