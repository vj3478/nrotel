import json
import requests
import csv

# Configuration
GRAPHQL_URL = "https://api.newrelic.com/graphql"
API_KEY = "YOUR_NEW_RELIC_API_KEY"  # Replace with your actual key
INPUT_JSON = "dashboard.json"
OUTPUT_CSV = "validated_dashboard_queries.csv"
BATCH_SIZE = 25

# ‚úÖ Extract and batch NRQL queries from widgets (supports list of accountIds)
def extract_and_batch_queries(json_file, batch_size=25):
    with open(json_file) as f:
        data = json.load(f)

    batched = []
    temp_batch = []

    pages = data["data"]["actor"]["entity"].get("pages", [])
    for page in pages:
        for widget in page.get("widgets", []):
            config = widget.get("rawConfiguration", {})
            account_id = config.get("accountId")
            queries = config.get("nrqlQueries", [])

            # Convert accountId to comma-separated string if it's a list
            if isinstance(account_id, list):
                account_id_str = ",".join(map(str, account_id))
            else:
                account_id_str = str(account_id)

            for q in queries:
                if "query" in q:
                    temp_batch.append({
                        "accountId": account_id_str,
                        "nrqlQuery": q["query"]
                    })
                    if len(temp_batch) == batch_size:
                        batched.append(temp_batch)
                        temp_batch = []

    if temp_batch:
        batched.append(temp_batch)

    return batched

# üß† Build GraphQL query string with aliases and support multiple account IDs
def build_graphql_query(batch):
    parts = []
    for i, row in enumerate(batch):
        alias = f"q{i}"
        account_list = [int(a.strip()) for a in row["accountId"].split(",")]
        nrql = row["nrqlQuery"].replace('"', '\\"').replace('\n', ' ').replace('\r', '')

        parts.append(f"""
            {alias}: actor {{
                nrql(accounts: {account_list}, query: "{nrql}") {{
                    results
                    metadata {{ eventTypes facets accounts }}
                }}
            }}
        """)
    return "query {\n" + "\n".join(parts) + "\n}"

# üîç Send GraphQL request and determine which queries fail
def verify_nrql_batches(batches):
    headers = {
        "Api-Key": API_KEY,
        "Content-Type": "application/json"
    }
    results = []

    for batch in batches:
        gql = build_graphql_query(batch)
        response = requests.post(GRAPHQL_URL, headers=headers, json={"query": gql})
        try:
            data = response.json()
            error_paths = {err["path"][0] for err in data.get("errors", [])} if "errors" in data else set()
        except Exception as e:
            print("Error parsing response:", e)
            error_paths = set(f"q{i}" for i in range(len(batch)))

        for i, row in enumerate(batch):
            alias = f"q{i}"
            row["result"] = "failed" if alias in error_paths else row["nrqlQuery"]
            results.append(row)

    return results

# üíæ Save validated results to CSV
def save_to_csv(data, path):
    with open(path, mode='w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)

# üöÄ Main function
def main():
    batches = extract_and_batch_queries(INPUT_JSON, BATCH_SIZE)
    validated = verify_nrql_batches(batches)
    save_to_csv(validated, OUTPUT_CSV)
    print(f"‚úÖ Validation complete. Output written to: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
